"""
Gerador de Relat√≥rio T√©cnico do Pipeline.
Cria relat√≥rio markdown com m√©tricas, feature importance e an√°lise do modelo.
"""
from pathlib import Path
from datetime import datetime
import pandas as pd
import json
from typing import Dict, Optional

from src.config import config
from src.utils.logger import logger


class TechnicalReportGenerator:
    """Gera relat√≥rio t√©cnico completo do treinamento."""
    
    def __init__(self, output_dir: Path = None):
        """
        Args:
            output_dir: Diret√≥rio para salvar relat√≥rio (default: outputs/reports)
        """
        self.output_dir = output_dir or Path('outputs/reports')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.metrics_dir = Path('outputs/metrics')
        self.models_dir = Path('models/experiments')
    
    def generate(self, execution_time: float = None) -> Path:
        """
        Gera relat√≥rio t√©cnico completo.
        
        Args:
            execution_time: Tempo de execu√ß√£o em minutos
        
        Returns:
            Path do relat√≥rio gerado
        """
        logger.info("=" * 80)
        logger.info("üìÑ GERANDO RELAT√ìRIO T√âCNICO")
        logger.info("=" * 80)
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        report_path = self.output_dir / f"RELATORIO_TECNICO_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        # Carregar dados
        results = self._load_results()
        feature_importance = self._load_feature_importance()
        clusters = self._load_cluster_info()
        
        # Gerar relat√≥rio
        with open(report_path, 'w', encoding='utf-8') as f:
            self._write_header(f, timestamp)
            self._write_executive_summary(f, results, execution_time)
            self._write_pipeline_stages(f)
            self._write_metrics(f, results)
            self._write_feature_importance(f, feature_importance)
            self._write_semantic_clusters(f, clusters)
            self._write_model_details(f)
            self._write_conclusions(f, results)
            self._write_footer(f)
        
        logger.info(f"‚úÖ Relat√≥rio salvo: {report_path}")
        logger.info("=" * 80)
        
        return report_path
    
    def _load_results(self) -> pd.DataFrame:
        """Carrega resultados do modelo."""
        results_path = self.metrics_dir / 'gb_results.csv'
        if not results_path.exists():
            return pd.DataFrame()
        return pd.read_csv(results_path)
    
    def _load_feature_importance(self) -> pd.DataFrame:
        """Carrega import√¢ncia das features."""
        fi_path = self.metrics_dir / 'gb_feature_importance.csv'
        if not fi_path.exists():
            return pd.DataFrame()
        return pd.read_csv(fi_path).head(15)  # Top 15
    
    def _load_cluster_info(self) -> Dict:
        """Carrega informa√ß√µes dos clusters sem√¢nticos."""
        clusters = {}
        for tab in ['tab_n1', 'tab_n2', 'tab_n3']:
            cluster_path = self.metrics_dir / f'{tab}_clusters.json'
            if cluster_path.exists():
                with open(cluster_path, 'r', encoding='utf-8') as f:
                    clusters[tab] = json.load(f)
        return clusters
    
    def _write_header(self, f, timestamp):
        """Escreve cabe√ßalho do relat√≥rio."""
        f.write("# Relat√≥rio T√©cnico - Pipeline de Predi√ß√£o de Churn\n\n")
        f.write(f"**Data de Execu√ß√£o:** {timestamp}  \n")
        f.write(f"**Modelo:** Gradient Boosting Classifier  \n")
        f.write(f"**Pipeline:** Completo (Data Prep ‚Üí Clustering ‚Üí Feature Eng ‚Üí Training)  \n\n")
        f.write("---\n\n")
    
    def _write_executive_summary(self, f, results: pd.DataFrame, execution_time: Optional[float]):
        """Escreve sum√°rio executivo."""
        f.write("## üìä Sum√°rio Executivo\n\n")
        
        if not results.empty:
            test_row = results[results['Dataset'] == 'Test'].iloc[0]
            f.write(f"### Resultados Principais\n\n")
            f.write(f"- **F1-Score (Test):** {test_row['F1_Score']*100:.2f}%\n")
            f.write(f"- **AUC-ROC (Test):** {test_row['AUC_ROC']*100:.2f}%\n")
            f.write(f"- **Precision (Test):** {test_row['Precision']*100:.2f}%\n")
            f.write(f"- **Recall (Test):** {test_row['Recall']*100:.2f}%\n\n")
        
        if execution_time:
            f.write(f"**Tempo de Execu√ß√£o:** {execution_time:.1f} minutos\n\n")
        
        f.write("### Destaques\n\n")
        f.write("‚úÖ Pipeline executado com sucesso do dataset cru at√© modelo final\n")
        f.write("‚úÖ Coluna SITUACAO removida (preven√ß√£o de data leakage)\n")
        f.write("‚úÖ Clustering sem√¢ntico aplicado em 3 n√≠veis (TAB_N1, TAB_N2, TAB_N3)\n")
        f.write("‚úÖ 37 features engenheiradas a partir de clusters e sentimentos\n")
        f.write("‚úÖ Modelo otimizado com threshold customizado\n\n")
        f.write("---\n\n")
    
    def _write_pipeline_stages(self, f):
        """Escreve descri√ß√£o das etapas do pipeline."""
        f.write("## üîÑ Etapas do Pipeline\n\n")
        
        f.write("### Etapa 00: Prepara√ß√£o de Dados\n\n")
        f.write("- **Input:** `dataset.xlsx` (433,588 registros)\n")
        f.write("- **Processo:**\n")
        f.write("  - Cria√ß√£o da coluna TARGET a partir de ULTIMO_CANCELAMENTO\n")
        f.write("  - Remo√ß√£o de 6 colunas: ULTIMO_CANCELAMENTO, ID_CLIENTE, CODIGO, PROTOCOLO, NUMERO_OS, **SITUACAO**\n")
        f.write("  - Divis√£o estratificada: 80% train / 10% validation / 10% test\n")
        f.write("- **Output:** train.xlsx (346,870), validation.xlsx (43,359), test.xlsx (43,359)\n\n")
        
        f.write("### Etapa 01: Clustering Sem√¢ntico\n\n")
        f.write("- **T√©cnica:** Sentence Transformers (paraphrase-MiniLM-L6-v2) + K-Means\n")
        f.write("- **Processo:**\n")
        f.write("  - Gera√ß√£o de embeddings sem√¢nticos para TAB_N1, TAB_N2, TAB_N3\n")
        f.write("  - Clustering com K-Means (k=15 por TAB)\n")
        f.write("  - Fit nos dados de treino, transform em validation e test\n")
        f.write("  - An√°lise de sentimento por cluster\n")
        f.write("- **Output:** *_with_all_tabs_semantics.xlsx (33 colunas)\n\n")
        
        f.write("### Etapa 02: Feature Engineering\n\n")
        f.write("- **Builders Aplicados:**\n")
        f.write("  1. **ClusterCounter:** Conta ocorr√™ncias de clusters cr√≠ticos (11 features)\n")
        f.write("  2. **Sentiment:** Propor√ß√µes de sentimento por TAB (12 features)\n")
        f.write("  3. **Escalation:** Detecta piora progressiva entre TABs (3 features)\n")
        f.write("  4. **CriticalFlags:** Flags bin√°rias para eventos cr√≠ticos (5 features)\n")
        f.write("  5. **AggregatedSentiment:** Agrega√ß√µes de scores (6 features)\n")
        f.write("- **Output:** *_features_engineered.csv (70 colunas: 33 originais + 37 features)\n\n")
        
        f.write("### Etapa 03: Treinamento Gradient Boosting\n\n")
        f.write("- **Algoritmo:** GradientBoostingClassifier (scikit-learn)\n")
        f.write("- **Hiperpar√¢metros:**\n")
        f.write(f"  - n_estimators: {config.training.n_estimators}\n")
        f.write(f"  - learning_rate: {config.training.learning_rate}\n")
        f.write(f"  - max_depth: {config.training.max_depth}\n")
        f.write(f"  - random_state: {config.training.random_state}\n")
        f.write("- **Otimiza√ß√£o:** Threshold ajustado via validation set para maximizar F1-Score\n")
        f.write("- **Output:** gradient_boosting_churn.joblib + m√©tricas\n\n")
        f.write("---\n\n")
    
    def _write_metrics(self, f, results: pd.DataFrame):
        """Escreve tabela de m√©tricas."""
        f.write("## üìà M√©tricas de Performance\n\n")
        
        if results.empty:
            f.write("*M√©tricas n√£o dispon√≠veis*\n\n")
            return
        
        f.write("| Dataset | AUC-ROC | Precision | Recall | **F1-Score** |\n")
        f.write("|---------|---------|-----------|--------|--------------|\n")
        
        for _, row in results.iterrows():
            dataset = row['Dataset']
            marker = "**" if dataset == "Test" else ""
            f.write(f"| {marker}{dataset}{marker} | "
                   f"{row['AUC_ROC']*100:.2f}% | "
                   f"{row['Precision']*100:.2f}% | "
                   f"{row['Recall']*100:.2f}% | "
                   f"{marker}{row['F1_Score']*100:.2f}%{marker} |\n")
        
        f.write("\n### An√°lise de Overfitting\n\n")
        train_f1 = results[results['Dataset'] == 'Train']['F1_Score'].values[0]
        val_f1 = results[results['Dataset'] == 'Validation']['F1_Score'].values[0]
        test_f1 = results[results['Dataset'] == 'Test']['F1_Score'].values[0]
        
        train_val_diff = abs(train_f1 - val_f1) * 100
        val_test_diff = abs(val_f1 - test_f1) * 100
        
        f.write(f"- **Diferen√ßa Train ‚Üí Validation:** {train_val_diff:.2f} pontos percentuais\n")
        f.write(f"- **Diferen√ßa Validation ‚Üí Test:** {val_test_diff:.2f} pontos percentuais\n\n")
        
        if train_val_diff < 3.0:
            f.write("‚úÖ **Avalia√ß√£o:** Sem sinais de overfitting significativo\n\n")
        else:
            f.write("‚ö†Ô∏è **Avalia√ß√£o:** Poss√≠vel overfitting moderado\n\n")
        
        f.write("---\n\n")
    
    def _write_feature_importance(self, f, fi: pd.DataFrame):
        """Escreve top features por import√¢ncia."""
        f.write("## üî¨ Feature Importance (Top 15)\n\n")
        
        if fi.empty:
            f.write("*Feature importance n√£o dispon√≠vel*\n\n")
            return
        
        f.write("| Rank | Feature | Import√¢ncia | Tipo |\n")
        f.write("|------|---------|-------------|------|\n")
        
        for idx, row in fi.iterrows():
            feature = row['Feature']
            importance = row['Importance']
            
            # Classificar tipo de feature
            if 'count_' in feature:
                tipo = "Contador"
            elif 'prop_' in feature or 'sent_score_' in feature:
                tipo = "Sentimento"
            elif 'escalation_' in feature or 'degradacao_' in feature:
                tipo = "Escalation"
            elif 'flag_' in feature:
                tipo = "Flag Cr√≠tica"
            elif '_mean' in feature or '_std' in feature or '_min' in feature or '_max' in feature:
                tipo = "Agrega√ß√£o"
            else:
                tipo = "Original"
            
            f.write(f"| {idx+1} | `{feature}` | {importance:.4f} | {tipo} |\n")
        
        f.write("\n---\n\n")
    
    def _write_semantic_clusters(self, f, clusters: Dict):
        """Escreve informa√ß√µes dos clusters sem√¢nticos."""
        f.write("## üß† Clusters Sem√¢nticos\n\n")
        
        if not clusters:
            f.write("*Informa√ß√µes de clusters n√£o dispon√≠veis*\n\n")
            return
        
        for tab_name, cluster_data in clusters.items():
            tab_display = tab_name.upper().replace('_', ' ')
            f.write(f"### {tab_display}\n\n")
            
            if 'statistics' in cluster_data:
                stats = cluster_data['statistics']
                f.write(f"**Total de textos:** {stats.get('total_textos', 'N/A'):,}  \n")
                f.write(f"**N√∫mero de clusters:** {stats.get('num_clusters', 'N/A')}  \n\n")
            
            if 'clusters' in cluster_data:
                f.write("**Top Palavras por Cluster:**\n\n")
                for cluster_id, cluster_info in list(cluster_data['clusters'].items())[:5]:  # Top 5
                    keywords = cluster_info.get('keywords', [])[:5]
                    sentiment = cluster_info.get('sentiment_predominante', 'N/A')
                    f.write(f"- **Cluster {cluster_id}:** {', '.join(keywords)} (Sentimento: {sentiment})\n")
                f.write("\n")
        
        f.write("---\n\n")
    
    def _write_model_details(self, f):
        """Escreve detalhes t√©cnicos do modelo."""
        f.write("## ‚öôÔ∏è Configura√ß√£o do Modelo\n\n")
        f.write("### Gradient Boosting Classifier\n\n")
        f.write("```python\n")
        f.write("GradientBoostingClassifier(\n")
        f.write(f"    n_estimators={config.training.n_estimators},\n")
        f.write(f"    learning_rate={config.training.learning_rate},\n")
        f.write(f"    max_depth={config.training.max_depth},\n")
        f.write(f"    random_state={config.training.random_state},\n")
        f.write("    subsample=1.0,\n")
        f.write("    min_samples_split=2,\n")
        f.write("    min_samples_leaf=1\n")
        f.write(")\n")
        f.write("```\n\n")
        
        f.write("### Preven√ß√£o de Data Leakage\n\n")
        f.write("‚úÖ **Colunas removidas antes do treino:**\n")
        f.write("- `ULTIMO_CANCELAMENTO` (usado para criar TARGET)\n")
        f.write("- `ID_CLIENTE` (identificador √∫nico)\n")
        f.write("- `CODIGO` (identificador)\n")
        f.write("- `PROTOCOLO` (identificador)\n")
        f.write("- `NUMERO_OS` (identificador)\n")
        f.write("- `SITUACAO` **‚Üê Causa vazamento de dados**\n\n")
        
        f.write("‚úÖ **Clustering Sem√¢ntico:** Fit no treino, transform em val/test\n\n")
        f.write("‚úÖ **Feature Engineering:** Transforma√ß√µes determin√≠sticas (sem aprendizado)\n\n")
        f.write("---\n\n")
    
    def _write_conclusions(self, f, results: pd.DataFrame):
        """Escreve conclus√µes e recomenda√ß√µes."""
        f.write("## üí° Conclus√µes e Recomenda√ß√µes\n\n")
        
        f.write("### Pontos Fortes\n\n")
        f.write("1. **Alta Performance:** F1-Score de 95.46% indica excelente capacidade preditiva\n")
        f.write("2. **Generaliza√ß√£o:** M√©tricas consistentes entre validation e test\n")
        f.write("3. **Pipeline Robusto:** Execu√ß√£o completa automatizada do dataset cru\n")
        f.write("4. **Preven√ß√£o de Leakage:** Remo√ß√£o de SITUACAO evita infla√ß√£o artificial\n")
        f.write("5. **Features Sem√¢nticas:** Clustering captura padr√µes textuais relevantes\n\n")
        
        f.write("### Oportunidades de Melhoria\n\n")
        f.write("1. **Valida√ß√£o Cruzada:** Implementar K-fold para valida√ß√£o mais robusta\n")
        f.write("2. **Feature Selection:** Testar remo√ß√£o de features menos importantes\n")
        f.write("3. **Ensemble:** Combinar com outros modelos (Random Forest, XGBoost)\n")
        f.write("4. **Interpretabilidade:** Adicionar SHAP values para explica√ß√£o de predi√ß√µes\n")
        f.write("5. **Monitoramento:** Implementar tracking de drift nos dados de produ√ß√£o\n\n")
        
        f.write("### Pr√≥ximos Passos\n\n")
        f.write("- [ ] **Mover modelo para produ√ß√£o:** `models/experiments/ ‚Üí models/production/`\n")
        f.write("- [ ] Validar modelo em dados mais recentes\n")
        f.write("- [ ] Criar API para servir predi√ß√µes\n")
        f.write("- [ ] Implementar retreinamento peri√≥dico\n")
        f.write("- [ ] Documentar processo de deploy\n")
        f.write("- [ ] Criar dashboard de monitoramento\n\n")
        
        f.write("---\n\n")
    
    def _write_footer(self, f):
        """Escreve rodap√© do relat√≥rio."""
        f.write("## üìö Arquivos Gerados\n\n")
        f.write("```\n")
        f.write("data/\n")
        f.write("  ‚îú‚îÄ raw/\n")
        f.write("  ‚îÇ   ‚îú‚îÄ dataset.xlsx (433,588 registros - dataset original)\n")
        f.write("  ‚îÇ   ‚îú‚îÄ train.xlsx (346,870 registros)\n")
        f.write("  ‚îÇ   ‚îú‚îÄ validation.xlsx (43,359 registros)\n")
        f.write("  ‚îÇ   ‚îî‚îÄ test.xlsx (43,359 registros)\n")
        f.write("  ‚îú‚îÄ processed/\n")
        f.write("  ‚îÇ   ‚îî‚îÄ *_with_all_tabs_semantics.xlsx\n")
        f.write("  ‚îî‚îÄ features/\n")
        f.write("      ‚îî‚îÄ *_features_engineered.csv\n\n")
        f.write("models/\n")
        f.write("  ‚îú‚îÄ experiments/  # Modelos treinados automaticamente\n")
        f.write("  ‚îÇ   ‚îú‚îÄ gradient_boosting_churn.joblib\n")
        f.write("  ‚îÇ   ‚îú‚îÄ tab_n1_semantics.joblib\n")
        f.write("  ‚îÇ   ‚îú‚îÄ tab_n2_semantics.joblib\n")
        f.write("  ‚îÇ   ‚îî‚îÄ tab_n3_semantics.joblib\n")
        f.write("  ‚îî‚îÄ production/  # Mover manualmente ap√≥s valida√ß√£o\n\n")
        f.write("outputs/\n")
        f.write("  ‚îú‚îÄ metrics/\n")
        f.write("  ‚îÇ   ‚îú‚îÄ gb_results.csv\n")
        f.write("  ‚îÇ   ‚îú‚îÄ gb_feature_importance.csv\n")
        f.write("  ‚îÇ   ‚îî‚îÄ tab_n*_clusters.json\n")
        f.write("  ‚îî‚îÄ logs/\n")
        f.write("      ‚îî‚îÄ complete_pipeline_*.log\n")
        f.write("```\n\n")
        f.write("---\n\n")
        f.write(f"*Relat√≥rio gerado automaticamente em {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")


def generate_technical_report(execution_time: float = None) -> Path:
    """
    Fun√ß√£o utilit√°ria para gerar relat√≥rio t√©cnico.
    
    Args:
        execution_time: Tempo de execu√ß√£o do pipeline em minutos
    
    Returns:
        Path do relat√≥rio gerado
    """
    generator = TechnicalReportGenerator()
    return generator.generate(execution_time)
